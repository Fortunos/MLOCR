 \documentclass{article}
    \usepackage[none]{hyphenat}
    \usepackage{amssymb}
    \usepackage{centernot}
    \usepackage{fixltx2e}
    \usepackage{dsfont}
    \usepackage{upgreek}
    \usepackage{amsmath}
    \usepackage{hhline}
    \usepackage{tipa}
    \usepackage{fancyhdr}
    \usepackage{graphicx}
    \usepackage{setspace}
    \graphicspath{ {images/} }
    \usepackage{amsmath,amssymb,amsthm, amscd}
\newcommand{\qedend}{\eqno\hfill\rule{1ex}{1ex}}
\renewcommand{\qed}{\hfill\rule{1ex}{1ex}}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\onehalfspacing
    \pagestyle{fancy}
    \fancyhead[L]{Report: MLOCR}
    \fancyhead[R]{Chet Aldrich and Laura Biester}
    \begin{document}
	 \section*{MLOCR}
	\label*{Optical Character Recognition using Machine Learning}
	   \subsection*{Introduction}
     Our original goal was the following: given a set of 60,000 handwritten digits, use a machine learning algorithm to make a computer learn how to identify and distinguish between them.

     At first, it was unclear where to start. However, we decided that we could treat the images as an aggregation of many pixels, which were much easier to measure concretely, and then use a machine learning algorithm to identify the number based on these features.

     There were a number of challenges with using this interpretation of the features of an image. First, it doesn't seem intuitive that given a bunch of pixel brightness values, we could determine what number was in the image. Second, it wasn't clear that such a myopic viewpoint would actually lead to accurate results. We could perhaps learn over time what pixel positions are more likely to be used if we are examining a 1, but what happens if we shift the position of the 1? Do the results change?

     Despite the lack of clarity as to whether such features would be realistic, we tried a few machine learning algorithms on the dataset to see if we could produce any meaningful results.
     \subsection*{Implementation}
	We started by getting data from the MMIST data set and figuring out how to process it. We ended up using a script that turned the MNIST data into a NumPy array. The arrays held images and their corresponding labels.
	
	Images were stored as 28x28 arrays of information about pixels, which were numbers from 0 to 1. For our project, we considered 0 to be a white pixel and anything above 0 to be a black pixel. From there, we created features based off of the placements of pixels on the screen.
	
	We then wrote two machine learning algorithms, Naive Bayes and Perceptron. We ran the data through those two classifiers, and compared their results. These two algorithms, with binary on/off features for each pixel, led to our best results.
	
	Over the course of the project, we also explored more classifiers and features. We tried a decision tree, but were not able to achieve above 50\% accuracy using all of the data, which made us feel that it was perhaps not the best classifier for our purposes. We also explored adding a feature to determine if there were any enclosed spaces in the number. However, our algorithm for finding enclosed spaces did not perform the way we has initially expected it to, because we were looking for ``on'' pixels to each side (NSEW) of an ``off'' pixel. We realized that this could be true in almost any number, excluding ones, because of curves. We decided that a more complicated implementation would slow down our code too much, so we did not include it in our final code.
     \subsection*{Results}
     \begin{table}[h!]
        \centering
        \begin{tabular}{l|l|l}
        Training Quantity & Naive Bayes (with tuning) & Perceptron (with tuning)   \\ \hline
        100               & 75.13\% (253.24 seconds)  & 66.45\% (90.84 seconds)    \\
        500               & 81.57\% (366.62 seconds)  & 80.49\% (141.57 seconds)   \\
        1000              & 82.23\% (354.90 seconds)  & 81.66\% (203.01 seconds)   \\
        5000              & 84.0\% (266.61 seconds)   & 84.10\% (1076.12 seconds)  \\
        10000             & 84.37\% (394.7 seconds)   & 84.64\% (2023.99 seconds)  \\
        20000             & 84.38\% (436.25 seconds)  & 85.44\% (4128.00 seconds)  \\
        30000             & 84.23\% (497.27 seconds)  & 88.62\% (5895.58 seconds)  \\
        40000             & 84.25\% (552.68 seconds)  & 88.26\% (7768.09 seconds)  \\
        50000             & 84.29\% (604.87 seconds)  & 89.14\% (10020.12 seconds) \\
        60000             & 84.31\% (632.77 seconds)  & 89.49\% (11683.14 seconds) \\
        \end{tabular}
     \end{table}
     We tried a significant number of different training quantities in order to see what was optimal given the training set and time. In terms of total percentage correct, the perceptron led with a slight edge over Naive Bayes with many training examples. However, you'll notice that the Naive Bayes algorithm was much more time efficient to reach a similar level of accuracy. A telling example of this phenomenon are the results with 5000 training instances. Both classifiers reach a similar level of accuracy, but the perceptron took nearly 5 times as much time to reach that level of accuracy. In fact, you can see this problem magnifies with larger amounts of training data.

     In the test setting, it was clear that Naive Bayes was much faster given large datasets. However, in many cases we would be allowed to train the classifier prior to using it on data, and this is where the perceptron shines. The classification of test data is very quick with the perceptron, and if we use pretrained values, we can have a more accurate classifier without much of a sacrifice in time other than the initial training time. In this case Naive Bayes loses its edge.
     \subsection*{Conclusion}



    \end{document}
