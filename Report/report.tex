 \documentclass{article}
    \usepackage[none]{hyphenat}
    \usepackage{amssymb}
    \usepackage{centernot}
    \usepackage{fixltx2e}
    \usepackage{dsfont}
    \usepackage{upgreek}
    \usepackage{amsmath}
    \usepackage{hhline}
    \usepackage{tipa}
    \usepackage{fancyhdr}
    \usepackage{graphicx}
    \graphicspath{ {images/} }
    \usepackage{amsmath,amssymb,amsthm, amscd}
\newcommand{\qedend}{\eqno\hfill\rule{1ex}{1ex}}
\renewcommand{\qed}{\hfill\rule{1ex}{1ex}}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

    \pagestyle{fancy}
    \fancyhead[L]{Proposal: MLOCR}
    \fancyhead[R]{Chet Aldrich and Laura Biester}
    \begin{document}
	 \section*{MLOCR}
	\label*{Optical Character Recognition using Machine Learning}
	   \subsection*{Introduction}
     Our original goal was the following: given a set of 60,000 handwritten digits, use a machine learning algorithm to make a computer learn how to identify and distinguish between them. \\

     At first, it was unclear where to start. However, we decided that we could treat the images as an aggregation of many pixels, which were much easier to measure concretely, and then use a machine learning algorithm to identify the number based on these features. \\

     There were a number of challenges with using this interpretation of the features of an image. First, it doesn't seem intuitive that given a bunch of pixel brightness values, we could determine what number was in the image. Second, it wasn't clear that such a myopic viewpoint would actually lead to accurate results. We could perhaps learn over time what pixel positions are more likely to be used if we are examining a 1, but what happens if we shift the position of the 1? Do the results change? \\

     Despite the lack of clarity as to whether such features would be realistic, we tried a few machine learning algorithms on the dataset to see if we could produce any meaningful results.
     \subsection*{Implementation}

     \subsection*{Results}
     \begin{table}[h!]
        \centering
        \begin{tabular}{l|l|l}
        Training Quantity & Naive Bayes (with tuning) & Perceptron (with tuning)  \\ \hline
        100               & 75.13\% (253.24 seconds)  & 66.45\% (90.84 seconds)   \\
        500               & 81.57\% (366.62 seconds)  & 80.49\% (141.57 seconds)  \\
        1000              & 82.23\% (354.90 seconds)  & 81.66\% (203.01 seconds)  \\
        5000              & 84.0\% (266.61 seconds)   & 84.10\% (1076.12 seconds) \\
        10000             & 84.37\% (394.7 seconds)   & 84.64\% (2023.99 seconds) \\
        20000             & 84.38\% (436.25 seconds)  & \% ( seconds)             \\
        30000             & 84.23\% (497.27 seconds)  & \% ( seconds)             \\
        40000             & 84.25\% (552.68 seconds)  & \% ( seconds)             \\
        50000             & 84.29\% (604.87 seconds)  & \% ( seconds)             \\
        60000             & 84.31\% (632.77 seconds)  & \% ( seconds)             \\
        \end{tabular}
     \end{table}
     \subsection*{Conclusion}



    \end{document}
