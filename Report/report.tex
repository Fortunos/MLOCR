 \documentclass{article}
    \usepackage[none]{hyphenat}
    \usepackage{amssymb}
    \usepackage{centernot}
    \usepackage{fixltx2e}
    \usepackage{dsfont}
    \usepackage{upgreek}
    \usepackage{amsmath}
    \usepackage{hhline}
    \usepackage{tipa}
    \usepackage{fancyhdr}
    \usepackage{graphicx}
    \graphicspath{ {images/} }
    \usepackage{amsmath,amssymb,amsthm, amscd}
\newcommand{\qedend}{\eqno\hfill\rule{1ex}{1ex}}
\renewcommand{\qed}{\hfill\rule{1ex}{1ex}}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

    \pagestyle{fancy}
    \fancyhead[L]{Proposal: MLOCR}
    \fancyhead[R]{Chet Aldrich and Laura Biester}
    \begin{document}
	 \section*{MLOCR}
	\label*{Optical Character Recognition using Machine Learning}
	   \subsection*{Introduction}
     Our original goal was the following: given a set of 60,000 handwritten digits, use a machine learning algorithm to make a computer learn how to identify and distinguish between them. \\

     At first, it was unclear where to start. However, we decided that we could treat the images as an aggregation of many pixels, which were much easier to measure concretely, and then use a machine learning algorithm to identify the number based on these features. \\

     There were a number of challenges with using this interpretation of the features of an image. First, it doesn't seem intuitive that given a bunch of pixel brightness values, we could determine what number was in the image. Second, it wasn't clear that such a myopic viewpoint would actually lead to accurate results. We could perhaps learn over time what pixel positions are more likely to be used if we are examining a 1, but what happens if we shift the position of the 1? Do the results change? \\

     Despite the lack of clarity as to whether such features would be realistic, we tried a few machine learning algorithms on the dataset to see if we could produce any meaningful results.
     \subsection*{Implementation}

     \subsection*{Results}
     \begin{table}[h!]
        \centering
        \begin{tabular}{l|l|l}
        Training Quantity & Naive Bayes (with tuning) & Perceptron (with tuning)  \\ \hline
        100               & 75.13\% (253.24 seconds)  & 66.45\% (90.84 seconds)   \\
        500               & 81.57\% (366.62 seconds)  & 80.49\% (141.57 seconds)  \\
        1000              & 82.23\% (354.90 seconds)  & 81.66\% (203.01 seconds)  \\
        5000              & 84.0\% (266.61 seconds)   & 84.10\% (1076.12 seconds) \\
        10000             & 84.37\% (394.7 seconds)   & 84.64\% (2023.99 seconds) \\
        20000             & 84.38\% (436.25 seconds)  & \% ( seconds)             \\
        30000             & 84.23\% (497.27 seconds)  & \% ( seconds)             \\
        40000             & 84.25\% (552.68 seconds)  & \% ( seconds)             \\
        50000             & 84.29\% (604.87 seconds)  & \% ( seconds)             \\
        60000             & 84.31\% (632.77 seconds)  & \% ( seconds)             \\
        \end{tabular}
     \end{table}
     We tried a significant number of different training quantities in order to see what was optimal given the training set and time. In terms of total percentage correct, the perceptron led with a slight edge over Naive Bayes with many training examples. However, you'll notice that the Naive Bayes algorithm was much more time efficient to reach a similar level of accuracy. A telling example of this phenomenon are the results with 5000 training instances. Both classifiers reach a similar level of accuracy, but the perceptron took nearly 5 times as much time to reach that level of accuracy. In fact, you can see this problem magnifies with larger amounts of training data.

     In the test setting, it was clear that Naive Bayes was much faster given large datasets. However, in many cases we would be allowed to train the classifier prior to using it on data, and this is where the perceptron shines. The classification of test data is very quick with the perceptron, and if we use pretrained values, we can have a more accurate classifier without much of a sacrifice in time other than the initial training time. In this case Naive Bayes loses its edge. 
     \subsection*{Conclusion}



    \end{document}
